{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NERC task with CoNLL 2003\n",
    "\n",
    "This is the first part of the task dedicated to reserching *Named Entity Recognition and Classification (NERC)* task applying to the *CoNLL 2003 dataset*. In this step we need find the best way to generate markup for the dataset in the subsequent steps. Also, we will experiment with prompt engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis\n",
    "\n",
    "First of all, we need to import necessery libraries and download our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import Counter, deque\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from dataclasses import dataclass\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://llm.ispras.ru/api/chat/completions\"\n",
    "API_MODEL_URL = \"https://llm.ispras.ru/api/models\"\n",
    "API_KEY = \"YOUR_TOKEN\"\n",
    "with open('./secrets') as file:\n",
    "    data: dict = json.load(file)\n",
    "    API_KEY = data.get('API_KEY', 'FAILED TO LOAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"eriktks/conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the information about the dataset. We have three splits: 'train', 'validation', 'test'.\n",
    "\n",
    "`dataset.keys()` - to see what splits we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_tags': [11, 13, 11, 12, 12],\n",
      " 'id': '666',\n",
      " 'ner_tags': [0, 0, 7, 0, 0],\n",
      " 'pos_tags': [24, 15, 16, 16, 21],\n",
      " 'tokens': ['Results', 'of', 'French', 'first', 'division']}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[\"test\"][666])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train':  14041 examples.\n",
      "Split 'validation':  3250 examples.\n",
      "Split 'test':  3453 examples.\n"
     ]
    }
   ],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset_split = dataset[split]\n",
    "    split_len = len(dataset_split)\n",
    "    print(f\"Split '{split}':  {split_len} examples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Entity Tags\n",
    "\n",
    "The original dataset uses named entity recognition (NER) tags in the IOB2 format.\n",
    "\n",
    "Each token is annotated with three types of tags:\n",
    "1. POS Tags: Indicate the token's grammatical role (e.g., 'NN', 'VB', etc.).\n",
    "2. Chunk Tags: Specify the syntactic chunk the token belongs to (e.g., 'B-NP', 'I-NP').\n",
    "3. NER Tags: Identify named entities using the IOB2 scheme:\n",
    "   - 'O'      : Token is not part of any entity.\n",
    "   - 'B-PER'  : Beginning of a person entity.\n",
    "   - 'I-PER'  : Inside a person entity.\n",
    "   - 'B-ORG'  : Beginning of an organization entity.\n",
    "   - 'I-ORG'  : Inside an organization entity.\n",
    "   - 'B-LOC'  : Beginning of a location entity.\n",
    "   - 'I-LOC'  : Inside a location entity.\n",
    "   - 'B-MISC' : Beginning of a miscellaneous entity.\n",
    "   - 'I-MISC' : Inside a miscellaneous entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGS = {'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n",
    " 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n",
    " 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n",
    " 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n",
    " 'WP': 44, 'WP$': 45, 'WRB': 46}\n",
    "\n",
    "CHUNK_TAGS = {'O': 0, 'B-ADJP': 1, 'I-ADJP': 2, 'B-AfdsdDVP': 3, 'I-ADVP': 4, 'B-CONJP': 5, 'I-CONJP': 6, 'B-INTJ': 7, 'I-INTJ': 8,\n",
    " 'B-LST': 9, 'I-LST': 10, 'B-NP': 11, 'I-NP': 12, 'B-PP': 13, 'I-PP': 14, 'B-PRT': 15, 'I-PRT': 16, 'B-SBAR': 17,\n",
    " 'I-SBAR': 18, 'B-UCP': 19, 'I-UCP': 20, 'B-VP': 21, 'I-VP': 22}\n",
    "\n",
    "NER_TAGS = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the distribution of tags in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags Distribution: Counter({22: 51545, 21: 34856, 11: 29962, 15: 28059, 12: 19773, 16: 17267, 24: 14580, 38: 12222, 7: 10898, 6: 10877, 37: 6304, 40: 5964, 30: 5852, 10: 5350, 35: 5193, 28: 4630, 4: 4233, 5: 4232, 39: 3769, 8: 3609, 42: 3439, 0: 3239, 27: 2323, 29: 2238, 41: 2132, 20: 1767, 23: 1010, 33: 784, 43: 769, 44: 769, 34: 642, 3: 622, 17: 579, 46: 551, 18: 388, 31: 259, 14: 228, 13: 210, 32: 62, 1: 60, 26: 47, 36: 42, 45: 41, 19: 37, 25: 5})\n",
      "Chunk Tags Distribution: Counter({12: 99175, 11: 85016, 0: 40828, 13: 27706, 21: 26510, 22: 12902, 3: 3846, 17: 1931, 1: 1747, 15: 784, 2: 318, 4: 210, 14: 135, 7: 103, 9: 64, 6: 59, 5: 47, 18: 30, 10: 4, 16: 2, 8: 1})\n",
      "NER Tags Distribution: Counter({0: 250660, 5: 10645, 1: 10059, 3: 9323, 2: 6991, 4: 5290, 7: 5062, 8: 1717, 6: 1671})\n"
     ]
    }
   ],
   "source": [
    "def analyze_tags(dataset):\n",
    "    pos_tags_counter = Counter()\n",
    "    chunk_tags_counter = Counter()\n",
    "    ner_tags_counter = Counter()\n",
    "    \n",
    "    for split in dataset.keys():\n",
    "        for example in dataset[split]:\n",
    "            pos_tags_counter.update(example[\"pos_tags\"])\n",
    "            chunk_tags_counter.update(example[\"chunk_tags\"])\n",
    "            ner_tags_counter.update(example[\"ner_tags\"])\n",
    "    \n",
    "    print(\"POS Tags Distribution:\", pos_tags_counter)\n",
    "    print(\"Chunk Tags Distribution:\", chunk_tags_counter)\n",
    "    print(\"NER Tags Distribution:\", ner_tags_counter)\n",
    "    \n",
    "    return pos_tags_counter, chunk_tags_counter, ner_tags_counter\n",
    "\n",
    "pos_tags_counter, chunk_tags_counter, ner_tags_counter = analyze_tags(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_pos_tags = {v: k for k, v in POS_TAGS.items()}\n",
    "inv_chunk_tags = {v: k for k, v in CHUNK_TAGS.items()}\n",
    "inv_ner_tags = {v: k for k, v in NER_TAGS.items()}\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 14))\n",
    "\n",
    "pos_sorted = sorted(pos_tags_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "pos_keys, pos_values = zip(*pos_sorted)\n",
    "pos_labels = [inv_pos_tags.get(k, str(k)) for k in pos_keys]\n",
    "\n",
    "# POS tags\n",
    "axes[0].bar(pos_labels, pos_values)\n",
    "axes[0].set_title(\"Distribution POS tags\")\n",
    "axes[0].set_xlabel(\"POS tag\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "chunk_sorted = sorted(chunk_tags_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "chunk_keys, chunk_values = zip(*chunk_sorted)\n",
    "chunk_labels = [inv_chunk_tags.get(k, str(k)) for k in chunk_keys]\n",
    "\n",
    "# Chunk tags\n",
    "axes[1].bar(chunk_labels, chunk_values)\n",
    "axes[1].set_title(\"Distribution Chunk tags\")\n",
    "axes[1].set_xlabel(\"Chunk tag\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# NER tags\n",
    "ner_sorted = sorted(ner_tags_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ner_keys, ner_values = zip(*ner_sorted)\n",
    "ner_labels = [inv_ner_tags.get(k, str(k)) for k in ner_keys]\n",
    "\n",
    "axes[2].bar(ner_labels, ner_values)\n",
    "axes[2].set_title(\"Distribution NER tags\")\n",
    "axes[2].set_xlabel(\"NER tag\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "axes[2].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence is split into tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data[\"train\"][\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use sentences in model we need to group tokens to lists for each sentences with function `get_sentence`.\n",
    "\n",
    "After generating we will have the batch with random sentences to send them to the model.\n",
    "\n",
    "The function `generate_corps` takes dataset_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(dataset_name: str, idx: int) -> str: \n",
    "    return ' '.join(dataset.data[dataset_name][\"tokens\"][idx].values.tolist())\n",
    "\n",
    "def generate_corps(size: int, dataset_name: str):\n",
    "    data = dataset[dataset_name]\n",
    "    data_size = data.shape[0]\n",
    "    return (get_sentence(dataset_name, idx) \n",
    "            for idx in random.choices(range(data_size), k=size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example for batch of 10 random sentences:\n",
    "\n",
    "```python\n",
    "generate_corps(10, \"train\"), sep='\\n'\n",
    "```\n",
    "\n",
    "we will have the batch of unannotated sentances.\n",
    "\n",
    "```bash\n",
    "delivered to consumer\n",
    "shares outstanding\n",
    "3 - Wayne Ferreira ( South Africa ) beat Jiri Novak ( Czech\n",
    "LECIVA PRAHA 2470.00 2470.00 1360 3359.200\n",
    "BOSTON AT CALIFORNIA\n",
    "-- Helsinki Newsroom +358 - 0 - 680 50 245\n",
    "More than 1,000 people have been executed in drug-related cases since the law took effect in 1989 .\n",
    "In another scene , a young girl performed oral sex with an unidentified adult man .\n",
    "Essex 532-8\n",
    "ACC sold 9.4 million tonnes in 1995/96 , retaining its top position in the Indian cement industry , Palkhivala said .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*generate_corps(2, \"train\"), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model requests\n",
    "\n",
    "To make the request to the models let's make the request's head and body. And see the available models list to use them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all models names:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_model_names():\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    response = requests.get(API_MODEL_URL, headers=headers)\n",
    "    if response.status_code // 100 == 2:\n",
    "        data = response.json()\n",
    "        models = data.get(\"data\", [])\n",
    "        model_names = [model[\"name\"] for model in models]\n",
    "        return model_names\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama3.3:latest',\n",
      " 'llama3.1:70b',\n",
      " 'llama3.1:405b',\n",
      " 'gemma2:27b',\n",
      " 'mistral-large:123b',\n",
      " 'command-r-plus:104b',\n",
      " 'llama3.1:8b',\n",
      " 'krith/qwen2.5-coder-32b-instruct:IQ3_M',\n",
      " 'deepseek-coder-v2:236b',\n",
      " 'llama3.2:latest',\n",
      " 'mistral:7b',\n",
      " 'RuadaptQwen2.5-32B-FuseO1:Q8',\n",
      " 'RuadaptQwen2.5-32B-Pro-Beta:Q8',\n",
      " 'RuadaptQwen2.5-32B-QWQ:Q8',\n",
      " 'deepseek-r1:14b',\n",
      " 'deepseek-r1:70b',\n",
      " 'deepseek-r1:7b',\n",
      " 'deepseek-r1:8b',\n",
      " 'qwen2.5-coder:1.5b',\n",
      " 'qwen2.5-coder:32b-instruct-q8_0']\n"
     ]
    }
   ],
   "source": [
    "model_names = get_all_model_names()\n",
    "\n",
    "pprint(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the body of request we construct several prompts with different description of the task to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifies named entities without specifying tag types\n",
    "prompt_1 = lambda sentence: f\"Classify all named entities in a sentence and categorize their semantic meaning: '{sentence}'\"\n",
    "\n",
    "# Classifies entities using a predefined set of tags\n",
    "prompt_2 = lambda sentence, tags: f\"Classify all named entities in a sentence: '{sentence}', based on following tags: {tags}\"\n",
    "\n",
    "# Uses POS, Chunk, and NER tags for comprehensive token analysis\n",
    "prompt_3 = lambda sentence, pos_tags, chunk_tags, ner_tags: f\"Classify all named entities in a sentence: '{sentence}', \\\n",
    "based on following parts of speech tags: {pos_tags}, \\\n",
    "based on following chunk tags: {chunk_tags}, \\\n",
    "based on following named entity recognition tags: {ner_tags}\"\n",
    "\n",
    "# Assigns POS, Chunk, and NER tags to each token\n",
    "prompt_4 = lambda sentence, pos_tags, chunk_tags, ner_tags: f\"Determine each entity that can be classified and assign them a pos_tag, a chunk_tag and a ner_tag, using following lists: \\\n",
    "parts of speech tags: [{pos_tags}], \\\n",
    "chunk tags: [{chunk_tags}], \\\n",
    "named entity recognition tags: [{ner_tags}] from the following: '{sentence}'.\" \n",
    "\n",
    "# Splits entities into tokens and assigns tags at token level\n",
    "prompt_4_1 = lambda sentence, pos_tags, chunk_tags, ner_tags: f\"Determine each entity that can be classified, split it into tokens and assign them a pos_tag, a chunk_tag and a ner_tag, using following lists: \\\n",
    "parts of speech tags: [{pos_tags}], \\\n",
    "chunk tags: [{chunk_tags}], \\\n",
    "named entity recognition tags: [{ner_tags}] from the following: '{sentence}'.\" \n",
    "\n",
    "# Processes multiple sentences, assigning tags to all tokens\n",
    "prompt_5 = lambda sentences, pos_tags, chunk_tags, ner_tags: f\"\"\"Determine each entity that can be classified and assign them a pos_tag, a chunk_tag and a ner_tag, using following lists: \\\n",
    "parts of speech tags: [{pos_tags}], \\\n",
    "chunk tags: [{chunk_tags}], \\\n",
    "named entity recognition tags: [{ner_tags}] from the following sentences: {', '.join(f\"'{sentence}'\" for sentence in sentences)}.\"\"\"\n",
    "\n",
    "# Returns JSON output with POS, Chunk, and NER tags\n",
    "prompt_6 = lambda sentence, pos_tags, chunk_tags, ner_tags: f\"\"\"Identify each named entity in the sentence and classify it by the following tags:\n",
    "    - POS tags: {pos_tags}\n",
    "    - Chunk tags: {chunk_tags}\n",
    "    - NER tags: {ner_tags}\n",
    "    Sentence: '{sentence}'\n",
    "    Return the response in JSON format with the fields: text, pos_tag, chunk_tag, ner_tag.\"\"\"\n",
    "\n",
    "# Analyzes token sequences, assigning tags with JSON output\n",
    "prompt_7 = lambda tokens, pos_tags, chunk_tags, ner_tags: f\"\"\"Identify each named entity from the following sequence of tokens and classify it by the following tags:\n",
    "    - POS tags: {pos_tags}\n",
    "    - Chunk tags: {chunk_tags}\n",
    "    - NER tags: {ner_tags}\n",
    "    Tokens: '{\", \".join(f\"`{token}`\" for token in tokens)}'\n",
    "    Return the response in JSON format with the fields: text, pos_tag, chunk_tag, ner_tag.\"\"\"\n",
    "\n",
    "prompt_7_1 = lambda tokens, pos_tags, chunk_tags, ner_tags: f\"\"\"Identify each named entity from the following sequence of tokens and classify it by the following tags, each token is enquoted with \"`\" and \"`\" is not part of token:\n",
    "    - POS tags: {pos_tags}\n",
    "    - Chunk tags: {chunk_tags}\n",
    "    - NER tags: {ner_tags}\n",
    "    Tokens: '{\", \".join(f\"`{token}`\" for token in tokens)}'\n",
    "    Return the response in JSON format with the fields: tokens, pos_tags, chunk_tags, ner_tags.\n",
    "    Each token has to be evaluated. The number of tags must match the number of tokens of each token type. \n",
    "    Example of the correct output:\n",
    "    {{'chunk_tags': [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0],\n",
    "    'ner_tags': [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'pos_tags': [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7],\n",
    "    'tokens': ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']}}\"\"\"\n",
    "\n",
    "prompt_7_1_1 = lambda tokens, ner_tags: f\"\"\"Identify each named entity from the following sequence of tokens and classify it by the NER tags. Each token is enquoted with \"`\" and \"`\" is not part of token:\n",
    "    - NER tags: {ner_tags}\n",
    "    Tokens: '{\", \".join(f\"`{token}`\" for token in tokens)}'\n",
    "    Return the response in JSON format with the fields: ner_tags, tokens.\n",
    "    Each token has to be evaluated. The number of tags must match the number of tokens of each token type. \n",
    "    Example of the correct output:\n",
    "    {{'ner_tags': [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'tokens': ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']}}\"\"\"\n",
    "\n",
    "\n",
    "# Multiple token sequences with strict JSON formatting.\n",
    "prompt_7_2 = lambda tokenss, pos_tags, chunk_tags, ner_tags: f\"\"\"Identify each named entity in each sequence of tokens from list of sequences of tokens and classify it by the following tags, each token is enquoted with \"`\" and \"`\" is not part of token and each sequence is enclosed in square brackets:\n",
    "    - POS tags: {pos_tags}\n",
    "    - Chunk tags: {chunk_tags}\n",
    "    - NER tags: {ner_tags}\n",
    "    Tokens: '{\", \".join(f\"[{', '.join(f'`{token}`' for token in tokens)}]\" for tokens in tokenss)}'\n",
    "    Return the response in JSON format as list of entries where entry has fields: pos_tags, chunk_tags, ner_tags, tokens.\n",
    "    Each token has to be evaluated. The number of tags of each token type must match the number of tokens in each entry exactly.\n",
    "    Example of the correct output:\n",
    "    [\n",
    "    {{'chunk_tags': [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0],\n",
    "    'ner_tags': [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    'pos_tags': [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7],\n",
    "    'tokens': ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']}}, \n",
    "    {{'chunk_tags': [11, 13, 11, 12, 12],\n",
    "    'ner_tags': [0, 0, 7, 0, 0],\n",
    "    'pos_tags': [24, 15, 16, 16, 21],\n",
    "    'tokens': ['Results', 'of', 'French', 'first', 'division']}}\n",
    "    ]\"\"\"\n",
    "\n",
    "# Expert task for multiple sequences with detailed JSON output\n",
    "prompt_7_3 = lambda tokenss, pos_tags, chunk_tags, ner_tags: f\"\"\"\n",
    "You are an expert in Named Entity Recognition (NER) and text annotation. Your task is to analyze each sequence of tokens, identify named entities, and classify them according to the following tags:\n",
    "\n",
    "### Tags:\n",
    "- **POS tags**: {pos_tags}\n",
    "- **Chunk tags**: {chunk_tags}\n",
    "- **NER tags**: {ner_tags}\n",
    "\n",
    "### Input:\n",
    "You are given a list of token sequences. Each token is enclosed in backticks (`), and each sequence is enclosed in square brackets. For example:\n",
    "`[`token1`, `token2`], [`token3`, `token4`]`\n",
    "\n",
    "### Task:\n",
    "1. For each token sequence:\n",
    "   - Analyze each token.\n",
    "   - Assign a POS tag, a Chunk tag, and a NER tag to each token.\n",
    "   - Ensure that the number of tags matches the number of tokens exactly.\n",
    "2. Return the result in JSON format as a list of entries. Each entry must have the following fields:\n",
    "   - `tokens`: List of tokens.\n",
    "   - `pos_tags`: List of POS tags for each token.\n",
    "   - `chunk_tags`: List of Chunk tags for each token.\n",
    "   - `ner_tags`: List of NER tags for each token.\n",
    "\n",
    "### Rules:\n",
    "- Each token must be evaluated.\n",
    "- The number of tags (POS, Chunk, NER) must match the number of tokens exactly.\n",
    "- Use the provided tag mappings for classification.\n",
    "\n",
    "### Example Input:\n",
    "`[`SOCCER`, `-`, `JAPAN`, `GET`, `LUCKY`, `WIN`, `,`, `CHINA`, `IN`, `SURPRISE`, `DEFEAT`, `.`], [`Results`, `of`, `French`, `first`, `division`]`\n",
    "\n",
    "### Example Output:\n",
    "```json\n",
    "[\n",
    "    {{\n",
    "        \"tokens\": [\"SOCCER\", \"-\", \"JAPAN\", \"GET\", \"LUCKY\", \"WIN\", \",\", \"CHINA\", \"IN\", \"SURPRISE\", \"DEFEAT\", \".\"],\n",
    "        \"pos_tags\": [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7],\n",
    "        \"chunk_tags\": [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0],\n",
    "        \"ner_tags\": [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    }},\n",
    "    {{\n",
    "        \"tokens\": [\"Results\", \"of\", \"French\", \"first\", \"division\"],\n",
    "        \"pos_tags\": [24, 15, 16, 16, 21],\n",
    "        \"chunk_tags\": [11, 13, 11, 12, 12],\n",
    "        \"ner_tags\": [0, 0, 7, 0, 0]\n",
    "    }}\n",
    "]\"\"\"\n",
    "\n",
    "# Only NER tagging for multiple sequences\n",
    "prompt_7_4 = lambda tokenss, ner_tags: f\"\"\"\n",
    "You are an expert in Named Entity Recognition (NER). Your task is to analyze each sequence of tokens, identify named entities, and classify them according to the following NER tags:\n",
    "\n",
    "### NER Tags:\n",
    "{ner_tags}\n",
    "\n",
    "### Input:\n",
    "You are given a list of token sequences. Each token is enclosed in backticks (`), and each sequence is enclosed in square brackets. For example:\n",
    "`[`token1`, `token2`], [`token3`, `token4`]`\n",
    "\n",
    "### Task:\n",
    "1. For each token sequence:\n",
    "   - Analyze each token.\n",
    "   - Assign a NER tag to each token.\n",
    "   - Ensure that the number of NER tags matches the number of tokens exactly.\n",
    "2. Return the result in JSON format as a list of entries. Each entry must have the following fields:\n",
    "   - `tokens`: List of tokens.\n",
    "   - `ner_tags`: List of NER tags for each token.\n",
    "\n",
    "### Rules:\n",
    "- Each token must be evaluated.\n",
    "- The number of NER tags must match the number of tokens exactly.\n",
    "- Use the provided NER tag mappings for classification.\n",
    "\n",
    "### Example Input:\n",
    "`[`SOCCER`, `-`, `JAPAN`, `GET`, `LUCKY`, `WIN`, `,`, `CHINA`, `IN`, `SURPRISE`, `DEFEAT`, `.`], [`Results`, `of`, `French`, `first`, `division`]`\n",
    "\n",
    "### Example Output:\n",
    "```json\n",
    "[\n",
    "    {{\n",
    "        \"tokens\": [\"SOCCER\", \"-\", \"JAPAN\", \"GET\", \"LUCKY\", \"WIN\", \",\", \"CHINA\", \"IN\", \"SURPRISE\", \"DEFEAT\", \".\"],\n",
    "        \"ner_tags\": [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    }},\n",
    "    {{\n",
    "        \"tokens\": [\"Results\", \"of\", \"French\", \"first\", \"division\"],\n",
    "        \"ner_tags\": [0, 0, 7, 0, 0]\n",
    "    }}\n",
    "]\"\"\"\n",
    "\n",
    "# NER tagging for a single token sequence\n",
    "prompt_8 = lambda tokens, ner_tags: f\"\"\"Identify each named entity from the following sequence of tokens and classify it by the NER tags:\n",
    "    - NER tags: {ner_tags}\n",
    "    Tokens: '{\", \".join(f\"`{token}`\" for token in tokens)}'\n",
    "    Return the response in JSON format with the fields: text, pos_tag, chunk_tag, ner_tag.\"\"\"\n",
    "\n",
    "# JSON output with text and NER tags only\n",
    "prompt_9 = lambda tokens, ner_tags: f\"\"\"Analyze the following sequence of tokens to identify and classify each named entity using the provided NER tags. Follow these instructions precisely:\n",
    "    NER Tags: {ner_tags}\n",
    "    Tokens: '{\", \".join(f\"{token}\" for token in tokens)}'\n",
    "    Return your results as a valid JSON array. Each object in the array must include exactly two fields:\n",
    "\n",
    "    text: The text of the identified named entity.\n",
    "    ner_tag: The corresponding NER tag from the list provided.\n",
    "    Do not include any extra information or commentary in your output.\"\"\"\n",
    "\n",
    "# JSON output with text and NER tags, emphasizing precision\n",
    "prompt_10 = lambda tokens, ner_tags: f\"\"\"Perform Named Entity Recognition (NER) on the following sequence of tokens. Identify each named entity and classify it using the provided NER tags.\n",
    "    NER Tags: {ner_tags}\n",
    "    Tokens: {\", \".join(f\"'{token}'\" for token in tokens)}\n",
    "    Return the response in a structured JSON format with the following fields for each entity:\n",
    "    text: The identified named entity (exact text from the tokens).\n",
    "    ner_tag: The corresponding NER tag for the entity.\n",
    "    Ensure the output is precise and adheres strictly to the provided NER tags and token sequence.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate the request's payload, accepting prompt as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_request(prompt: str, model: str = \"llama3.3:latest\"):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def perform_requests_parallel(prompts, model: str = \"llama3.3:latest\"):\n",
    "    with multiprocessing.Pool(10) as p:\n",
    "        results = p.starmap(perform_request, [(prompt, model) for prompt in prompts])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example response with prompt_4 we get using perform_request:\n",
    "\n",
    "```python\n",
    "single_sentence = tuple(generate_corps(1, \"train\"))\n",
    "single_prompt = prompt_4(single_sentence, POS_TAGS, CHUNK_TAGS, NER_TAGS)\n",
    "\n",
    "response = perform_request(single_prompt)\n",
    "pprint(response)\n",
    "pprint((json.loads(response[\"choices\"][0][\"message\"][\"content\"]), single_prompt))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make multiprocessing request to make the responces from model faster (if only not the server allow to perform requests from one API one-by-one).\n",
    "\n",
    "```python\n",
    "single_sentence = tuple(generate_corps(1, \"train\"))\n",
    "single_prompt = prompt_4(single_sentence, POS_TAGS, CHUNK_TAGS, NER_TAGS)\n",
    "\n",
    "response = perform_request(single_prompt)\n",
    "pprint(response)\n",
    "pprint((json.loads(response[\"choices\"][0][\"message\"][\"content\"]), single_prompt))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform into dictionary the results we got from the prompt_5:\n",
    "\n",
    "(In the prompt_5 we are trying to send to the model multiply sentences in a single request)\n",
    "\n",
    "```python\n",
    "pprint([(json.loads(response[\"choices\"][0][\"message\"][\"content\"] ), corp) for response, corp in zip(responses, corps)])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending model too much sentences to execute maybe too long, this prompt example was working more than 10 minutes:\n",
    "\n",
    "```python\n",
    "prompts = tuple(prompt_4_1(sentence, POS_TAGS, CHUNK_TAGS, NER_TAGS) for sentence in corps)\n",
    "with multiprocessing.Pool(10) as p: \n",
    "    responses = p.map(perform_request, prompts)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same we can test our model with prompt_6:\n",
    "\n",
    "```python\n",
    "prompts = tuple(prompt_6(corp, POS_TAGS, CHUNK_TAGS, NER_TAGS) for corp in corps )\n",
    "with multiprocessing.Pool(10) as p: \n",
    "    responses = p.map(perform_request, prompts)\n",
    "\n",
    "pprint([(json.loads(response[\"choices\"][0][\"message\"][\"content\"] ), corp) for response, corp in zip(responses, corps)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate free metrics from the sklearn: `precision`, `recall`, `f1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_labels, predicted_labels):\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro', zero_division=0)\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro', zero_division=0)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a SHIFT for the random sentences to appear while testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHIFT = random.randint(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use implemented metrics we will generate true metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics_data(dataset, model, num_examples=10):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    for i in map(lambda x: x + SHIFT, range(num_examples)):\n",
    "        # print(i)\n",
    "        # sentence = ' '.join(dataset[\"test\"][i][\"tokens\"])\n",
    "        tokens = dataset[\"test\"][i][\"tokens\"]\n",
    "        \n",
    "        prompt = prompt_7_1(tokens, POS_TAGS, CHUNK_TAGS, NER_TAGS)\n",
    "        # prompt = prompt_9(tokens, NER_TAGS)\n",
    "        response = perform_request(prompt, model)\n",
    "        \n",
    "        if response and \"choices\" in response:\n",
    "            predicted_entities = json.loads(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "            true_entities = dataset[\"test\"][i]\n",
    "            predicted_labels.extend(predicted_entities[\"ner_tags\"])\n",
    "\n",
    "            true_labels.extend(true_entities[\"ner_tags\"])\n",
    "    \n",
    "    return true_labels, predicted_labels\n",
    "\n",
    "def print_metrics(true_labels, predicted_labels):\n",
    "    precision, recall, f1 = calculate_metrics(true_labels, predicted_labels)\n",
    "    print(f\"Metrics on test set:\\nPrecision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "\n",
    "# Variations of the original function generate_metrics_data to use it with prompts with concateneted string OR parallel prompting\n",
    "def generate_metrics_data_prompt_many(dataset, model, num_examples=10):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    indicies = tuple(map(lambda x: x + SHIFT, range(num_examples)))\n",
    "    tokenss = [dataset[\"test\"][i][\"tokens\"] for i in indicies]\n",
    "\n",
    "    prompt = prompt_7_2(tokenss, POS_TAGS, CHUNK_TAGS, NER_TAGS)\n",
    "    response = perform_request(prompt, model)\n",
    "    pprint(response)\n",
    "\n",
    "    if response and \"choices\" in response:\n",
    "        shlepa = json.loads(response['choices'][0]['message']['content'])\n",
    "        answers = next(iter(shlepa.values()))\n",
    "        for i, predicted_entities in zip(indicies, answers):\n",
    "            true_entities = dataset[\"test\"][i]\n",
    "            predicted_labels.extend(predicted_entities[\"ner_tags\"])\n",
    "            true_labels.extend(true_entities[\"ner_tags\"])\n",
    "    \n",
    "    return true_labels, predicted_labels\n",
    "\n",
    "\n",
    "def generate_metrics_data_parallel(dataset, model, num_examples=10):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    responses = perform_requests_parallel((prompt_7_1(dataset[\"test\"][i][\"tokens\"], POS_TAGS, CHUNK_TAGS, NER_TAGS) for i in map(lambda x: x + SHIFT, range(num_examples))), model)\n",
    "    \n",
    "    for response in responses:        \n",
    "        if response and \"choices\" in response:\n",
    "            predicted_entities = json.loads(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "            true_entities = dataset[\"test\"][i]\n",
    "            # print(\"----- Predicted: \")\n",
    "            # pprint(predicted_entities)\n",
    "            # print(\"----- True: \")\n",
    "            # pprint(true_entities)\n",
    "            # pprint(tuple(itertools.zip_longest(true_entities['tokens'], predicted_entities['tokens'], fillvalue=None)))\n",
    "            # pprint(tuple(itertools.zip_longest(true_entities[\"ner_tags\"], predicted_entities[\"ner_tags\"], fillvalue=None)))\n",
    "\n",
    "            # print(tuple(len(t) for t in true_entities.values()))\n",
    "            # print(tuple(len(t) for t in predicted_entities.values()))\n",
    "\n",
    "            # entities_list = predicted_entities.get(\"entities\", [])\n",
    "            predicted_labels.extend(predicted_entities[\"ner_tags\"])\n",
    "\n",
    "            true_labels.extend(true_entities[\"ner_tags\"])\n",
    "    \n",
    "    return true_labels, predicted_labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
