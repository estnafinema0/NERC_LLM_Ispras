{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NERC task with CoNLL 2003\n",
    "\n",
    "This is the first part of the task dedicated to reserching *Named Entity Recognition and Classification (NERC)* task applying to the *CoNLL 2003 dataset*. In this step we need find the best way to generate markup for the dataset in the subsequent steps. Also, we will experiment with prompt engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis\n",
    "\n",
    "First of all, we need to import necessery libraries and download our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://llm.ispras.ru/api/chat/completions\"\n",
    "API_KEY = \"YOUR_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"eriktks/conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the information about the dataset. We have three splits: 'train', 'validation', 'test'.\n",
    "\n",
    "`dataset.keys()` - to see what splits we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_tags': [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0],\n",
      " 'id': '0',\n",
      " 'ner_tags': [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      " 'pos_tags': [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7],\n",
      " 'tokens': ['SOCCER',\n",
      "            '-',\n",
      "            'JAPAN',\n",
      "            'GET',\n",
      "            'LUCKY',\n",
      "            'WIN',\n",
      "            ',',\n",
      "            'CHINA',\n",
      "            'IN',\n",
      "            'SURPRISE',\n",
      "            'DEFEAT',\n",
      "            '.']}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(dataset_name: str, idx: int) -> str: \n",
    "    return ' '.join(dataset.data[dataset_name][\"tokens\"][idx].values.tolist())\n",
    "\n",
    "def generate_corps(size: int, dataset_name: str):\n",
    "    data = dataset[dataset_name]\n",
    "    data_size = data.shape[0]\n",
    "    return (get_sentence(dataset_name, idx) \n",
    "            for idx in random.choices(range(data_size), k=size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delivered to consumer\n",
      "shares outstanding\n",
      "3 - Wayne Ferreira ( South Africa ) beat Jiri Novak ( Czech\n",
      "LECIVA PRAHA 2470.00 2470.00 1360 3359.200\n",
      "BOSTON AT CALIFORNIA\n",
      "-- Helsinki Newsroom +358 - 0 - 680 50 245\n",
      "More than 1,000 people have been executed in drug-related cases since the law took effect in 1989 .\n",
      "In another scene , a young girl performed oral sex with an unidentified adult man .\n",
      "Essex 532-8\n",
      "ACC sold 9.4 million tonnes in 1995/96 , retaining its top position in the Indian cement industry , Palkhivala said .\n"
     ]
    }
   ],
   "source": [
    "print(*generate_corps(10, \"train\"), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train':  14041 examples.\n",
      "Split 'validation':  3250 examples.\n",
      "Split 'test':  3453 examples.\n"
     ]
    }
   ],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset_split = dataset[split]\n",
    "    split_len = len(dataset_split)\n",
    "    print(f\"Split '{split}':  {split_len} examples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset features in the training split:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset features in the training split:\n",
      "{'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None), 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's structure output data as json to make structured requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\": \"llama3.3:latest\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Classify all named entities in a sentence and categorize their semantic meaning: '{sentence}'\"\n",
    "        } \n",
    "        for sentence in generate_corps(2, \"train\")\n",
    "    ],\n",
    "    \"format\": \"json\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(API_URL, headers=headers, json=payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format': 'json',\n",
      " 'messages': [{'content': 'Classify all named entities in a sentence and '\n",
      "                          \"categorize their semantic meaning: 'The won rose \"\n",
      "                          'against the dollar on Friday as banks unwound '\n",
      "                          'dollar positions on the belief that the won would '\n",
      "                          \"continue to strengthen , dealers said .'\",\n",
      "               'role': 'user'},\n",
      "              {'content': 'Classify all named entities in a sentence and '\n",
      "                          \"categorize their semantic meaning: 'Add Men 's \"\n",
      "                          \"singles , second round'\",\n",
      "               'role': 'user'}],\n",
      " 'model': 'llama3.3:latest'}\n"
     ]
    }
   ],
   "source": [
    "pprint(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"id\":\"llama3.3:latest-28035d26-0682-4392-870e-3d7c7a5c6c7e\",\"created\":1740772237,\"model\":\"llama3.3:latest\",\"choices\":[{\"index\":0,\"logprobs\":null,\"finish_reason\":\"stop\",\"message\":{\"content\":\"{ '\n",
      " '\\\\n  \\\\\"entities\\\\\": [\\\\n    {\\\\n      \\\\\"entity\\\\\": \\\\\"Men\\'s '\n",
      " 'singles\\\\\",\\\\n      \\\\\"type\\\\\": \\\\\"Event\\\\\",\\\\n      \\\\\"semantic meaning\\\\\": '\n",
      " '\\\\\"A tennis tournament category\\\\\"\\\\n    },\\\\n    {\\\\n      \\\\\"entity\\\\\": '\n",
      " '\\\\\"second round\\\\\",\\\\n      \\\\\"type\\\\\": \\\\\"Phase\\\\\",\\\\n      \\\\\"semantic '\n",
      " 'meaning\\\\\": \\\\\"Stage of a competition or process\\\\\"\\\\n    }\\\\n  ]\\\\n}\\\\n\\\\n '\n",
      " '\\\\n  \\\\n\\\\n\\\\n\\\\n  \\\\n \\\\n\\\\n  \\\\n\\\\n\\\\n\\\\n\\\\n\\\\n  \\\\n \\\\n '\n",
      " '\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n \\\\n\\\\n\\\\n \\\\n\\\\n \\\\n   \\\\n\\\\n\\\\n '\n",
      " '\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n \\\\n \\\\n\\\\n\\\\n\\\\n \\\\n\\\\n\\\\n\\\\n\\\\n\\\\n \\\\n '\n",
      " '\\\\n\\\\n \\\\n \\\\n\\\\n\\\\n  \\\\n\\\\n \\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n '\n",
      " '\\\\n\",\"role\":\"assistant\"}}],\"object\":\"chat.completion\",\"usage\":{\"response_token/s\":\"N/A\",\"prompt_token/s\":\"N/A\",\"total_duration\":0,\"load_duration\":0,\"prompt_eval_count\":0,\"prompt_tokens\":0,\"prompt_eval_duration\":0,\"eval_count\":0,\"completion_tokens\":0,\"eval_duration\":0,\"approximate_total\":\"0h0m0s\",\"total_tokens\":0,\"completion_tokens_details\":{\"reasoning_tokens\":0,\"accepted_prediction_tokens\":0,\"rejected_prediction_tokens\":0}}}')\n"
     ]
    }
   ],
   "source": [
    "response_json = response.json()\n",
    "pprint(response.text)\n",
    "\n",
    "text = response_json[\"choices\"][0][\"message\"][\"content\"] \n",
    "texts = [text[\"message\"][\"content\"] for text in response_json[\"choices\"]]\n",
    "# pprint(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform into dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entities': [{'entity': \"Men's singles\",\n",
      "               'semantic meaning': 'A tennis tournament category',\n",
      "               'type': 'Event'},\n",
      "              {'entity': 'second round',\n",
      "               'semantic meaning': 'Stage of a competition or process',\n",
      "               'type': 'Phase'}]}\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    pprint(json.loads(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
