{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NERC task with CoNLL 2003\n",
    "\n",
    "This is the first part of the task dedicated to reserching *Named Entity Recognition and Classification (NERC)* task applying to the *CoNLL 2003 dataset*. In this step we need find the best way to generate markup for the dataset in the subsequent steps. Also, we will experiment with prompt engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis\n",
    "\n",
    "First of all, we need to import necessery libraries and download our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import json\n",
    "import random\n",
    "import multiprocessing\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://llm.ispras.ru/api/chat/completions\"\n",
    "API_MODEL_URL = \"https://llm.ispras.ru/api/models\"\n",
    "API_KEY = \"YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"eriktks/conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the information about the dataset. We have three splits: 'train', 'validation', 'test'.\n",
    "\n",
    "`dataset.keys()` - to see what splits we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_tags': [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0],\n",
      " 'id': '0',\n",
      " 'ner_tags': [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      " 'pos_tags': [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7],\n",
      " 'tokens': ['SOCCER',\n",
      "            '-',\n",
      "            'JAPAN',\n",
      "            'GET',\n",
      "            'LUCKY',\n",
      "            'WIN',\n",
      "            ',',\n",
      "            'CHINA',\n",
      "            'IN',\n",
      "            'SURPRISE',\n",
      "            'DEFEAT',\n",
      "            '.']}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train':  14041 examples.\n",
      "Split 'validation':  3250 examples.\n",
      "Split 'test':  3453 examples.\n"
     ]
    }
   ],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset_split = dataset[split]\n",
    "    split_len = len(dataset_split)\n",
    "    print(f\"Split '{split}':  {split_len} examples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Tags\n",
    "\n",
    "The original dataset uses named entity recognition (NER) tags in the IOB2 format.\n",
    "\n",
    "Each token is annotated with three types of tags:\n",
    "1. POS Tags: Indicate the token's grammatical role (e.g., 'NN', 'VB', etc.).\n",
    "2. Chunk Tags: Specify the syntactic chunk the token belongs to (e.g., 'B-NP', 'I-NP').\n",
    "3. NER Tags: Identify named entities using the IOB2 scheme:\n",
    "   - 'O'      : Token is not part of any entity.\n",
    "   - 'B-PER'  : Beginning of a person entity.\n",
    "   - 'I-PER'  : Inside a person entity.\n",
    "   - 'B-ORG'  : Beginning of an organization entity.\n",
    "   - 'I-ORG'  : Inside an organization entity.\n",
    "   - 'B-LOC'  : Beginning of a location entity.\n",
    "   - 'I-LOC'  : Inside a location entity.\n",
    "   - 'B-MISC' : Beginning of a miscellaneous entity.\n",
    "   - 'I-MISC' : Inside a miscellaneous entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = {'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n",
    " 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n",
    " 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n",
    " 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n",
    " 'WP': 44, 'WP$': 45, 'WRB': 46}\n",
    "\n",
    "chunk_tags = {'O': 0, 'B-ADJP': 1, 'I-ADJP': 2, 'B-ADVP': 3, 'I-ADVP': 4, 'B-CONJP': 5, 'I-CONJP': 6, 'B-INTJ': 7, 'I-INTJ': 8,\n",
    " 'B-LST': 9, 'I-LST': 10, 'B-NP': 11, 'I-NP': 12, 'B-PP': 13, 'I-PP': 14, 'B-PRT': 15, 'I-PRT': 16, 'B-SBAR': 17,\n",
    " 'I-SBAR': 18, 'B-UCP': 19, 'I-UCP': 20, 'B-VP': 21, 'I-VP': 22}\n",
    "\n",
    "ner_tags = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence is split into tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.ListScalar: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[\"train\"][\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use sentences in model we need to group tokens to lists for each sentences with function `get_sentence`.\n",
    "\n",
    "After generating we will have the batch with random sentences to send them to the model.\n",
    "\n",
    "The function `generate_corps` takes dataset_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(dataset_name: str, idx: int) -> str: \n",
    "    return ' '.join(dataset.data[dataset_name][\"tokens\"][idx].values.tolist())\n",
    "\n",
    "def generate_corps(size: int, dataset_name: str):\n",
    "    data = dataset[dataset_name]\n",
    "    data_size = data.shape[0]\n",
    "    return (get_sentence(dataset_name, idx) \n",
    "            for idx in random.choices(range(data_size), k=size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example for batch of 10 random sentences:\n",
    "\n",
    "```python\n",
    "generate_corps(10, \"train\"), sep='\\n'\n",
    "```\n",
    "\n",
    "we will have the batch of unannotated sentances.\n",
    "\n",
    "```bash\n",
    "delivered to consumer\n",
    "shares outstanding\n",
    "3 - Wayne Ferreira ( South Africa ) beat Jiri Novak ( Czech\n",
    "LECIVA PRAHA 2470.00 2470.00 1360 3359.200\n",
    "BOSTON AT CALIFORNIA\n",
    "-- Helsinki Newsroom +358 - 0 - 680 50 245\n",
    "More than 1,000 people have been executed in drug-related cases since the law took effect in 1989 .\n",
    "In another scene , a young girl performed oral sex with an unidentified adult man .\n",
    "Essex 532-8\n",
    "ACC sold 9.4 million tonnes in 1995/96 , retaining its top position in the Indian cement industry , Palkhivala said .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ninety-day bank bill rates shed five points to 9.93 percent and September bank bill futures rose four to 90.18 .\n",
      "The facility has a tenor of six months .\n"
     ]
    }
   ],
   "source": [
    "print(*generate_corps(2, \"train\"), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model requests\n",
    "\n",
    "To make the request to the models let's make the request's head and body. And see the available models list to use them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve all models names:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_model_names():\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    response = requests.get(API_MODEL_URL, headers=headers)\n",
    "    if response.status_code // 100 == 2:\n",
    "        data = response.json()\n",
    "        models = data.get(\"data\", [])\n",
    "        model_names = [model[\"name\"] for model in models]\n",
    "        return model_names\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_model_names():\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    response = requests.get(API_MODEL_URL, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        models = data.get(\"data\", [])\n",
    "        model_names = [model[\"name\"] for model in models]\n",
    "        return model_names\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama3.3:latest',\n",
      " 'llama3.1:70b',\n",
      " 'llama3.1:405b',\n",
      " 'gemma2:27b',\n",
      " 'mistral-large:123b',\n",
      " 'command-r-plus:104b',\n",
      " 'llama3.1:8b',\n",
      " 'krith/qwen2.5-coder-32b-instruct:IQ3_M',\n",
      " 'deepseek-coder-v2:236b',\n",
      " 'llama3.2:latest',\n",
      " 'mistral:7b',\n",
      " 'RuadaptQwen2.5-32B-Pro-Beta:Q8',\n",
      " 'deepseek-r1:14b',\n",
      " 'deepseek-r1:70b',\n",
      " 'deepseek-r1:7b',\n",
      " 'deepseek-r1:8b',\n",
      " 'qwen2.5-coder:1.5b',\n",
      " 'qwen2.5-coder:32b-instruct-q8_0']\n"
     ]
    }
   ],
   "source": [
    "model_names = get_all_model_names()\n",
    "\n",
    "pprint(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the body of request we construct several prompts with different description of the task to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = lambda sentence: f\"Classify all named entities in a sentence and categorize their semantic meaning: '{sentence}'\"\n",
    "\n",
    "prompt_2 = lambda sentence, tags: f\"Classify all named entities in a sentence: '{sentence}', based on following tags: {tags}\"\n",
    "\n",
    "prompt_3 = lambda sentence, pos_tags, chunk_tags, ner_tags: f\"Classify all named entities in a sentence: '{sentence}', \\\n",
    "based on following parts of speech tags: {pos_tags}, \\\n",
    "based on following chunk tags: {chunk_tags}, \\\n",
    "based on following named entity recognition tags: {ner_tags}\"\n",
    "\n",
    "prompt_4 = lambda sentence, pos_tags, chunk_tags, ner_tags: f\"Determine each entity that can be classified and assign them a pos_tag, a chunk_tag and a ner_tag, using following lists: \\\n",
    "parts of speech tags: [{pos_tags}], \\\n",
    "chunk tags: [{chunk_tags}], \\\n",
    "named entity recognition tags: [{ner_tags}] from the following: '{sentence}'.\" \n",
    "\n",
    "prompt_5 = lambda sentences, pos_tags, chunk_tags, ner_tags: f\"\"\"Determine each entity that can be classified and assign them a pos_tag, a chunk_tag and a ner_tag, using following lists: \\\n",
    "parts of speech tags: [{pos_tags}], \\\n",
    "chunk tags: [{chunk_tags}], \\\n",
    "named entity recognition tags: [{ner_tags}] from the following sentences: {', '.join(f\"'{sentence}'\" for sentence in sentences)}.\"\"\"\n",
    "\n",
    "prompt_4_1 = lambda sentence, pos_tags, chunk_tags, ner_tags: f\"Determine each entity that can be classified, split it into tokens and assign them a pos_tag, a chunk_tag and a ner_tag, using following lists: \\\n",
    "parts of speech tags: [{pos_tags}], \\\n",
    "chunk tags: [{chunk_tags}], \\\n",
    "named entity recognition tags: [{ner_tags}] from the following: '{sentence}'.\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate the request's payload, accepting prompt as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_payload(prompt):\n",
    "    return {\n",
    "        \"model\": \"llama3.3:latest\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            } \n",
    "        ],\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "def perform_request(prompt: str):\n",
    "    return (\n",
    "        requests.post(API_URL, headers=headers, json=gen_payload(prompt))\n",
    "        .json()\n",
    "    )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example response we get using perform_request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'entities': [{'chunk_tag': 'B-NP',\n",
      "                'entity': '<generator',\n",
      "                'ner_tag': 'O',\n",
      "                'pos_tag': 'NN'},\n",
      "               {'chunk_tag': 'I-NP',\n",
      "                'entity': 'object',\n",
      "                'ner_tag': 'O',\n",
      "                'pos_tag': 'NN'},\n",
      "               {'chunk_tag': 'I-NP',\n",
      "                'entity': 'generate_corps',\n",
      "                'ner_tag': 'O',\n",
      "                'pos_tag': 'NN'},\n",
      "               {'chunk_tag': 'I-NP',\n",
      "                'entity': '<locals>',\n",
      "                'ner_tag': 'O',\n",
      "                'pos_tag': 'NN'},\n",
      "               {'chunk_tag': 'I-NP',\n",
      "                'entity': '<genexpr>',\n",
      "                'ner_tag': 'O',\n",
      "                'pos_tag': 'NN'},\n",
      "               {'chunk_tag': 'B-PP',\n",
      "                'entity': 'at',\n",
      "                'ner_tag': 'O',\n",
      "                'pos_tag': 'IN'},\n",
      "               {'chunk_tag': 'I-NP',\n",
      "                'entity': '0x7fb140311f50',\n",
      "                'ner_tag': 'O',\n",
      "                'pos_tag': 'CD'}]},\n",
      " 'Determine each entity that can be classified and assign them a pos_tag, a '\n",
      " 'chunk_tag and a ner_tag, using following lists: parts of speech tags: '\n",
      " '[{\\'\"\\': 0, \"\\'\\'\": 1, \\'#\\': 2, \\'$\\': 3, \\'(\\': 4, \\')\\': 5, \\',\\': 6, '\n",
      " \"'.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12, 'EX': 13, 'FW': 14, \"\n",
      " \"'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, \"\n",
      " \"'NNP': 22, 'NNPS': 23, 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': \"\n",
      " \"28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33, 'SYM': 34, 'TO': \"\n",
      " \"35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': \"\n",
      " \"42, 'WDT': 43, 'WP': 44, 'WP$': 45, 'WRB': 46}], chunk tags: [{'O': 0, \"\n",
      " \"'B-ADJP': 1, 'I-ADJP': 2, 'B-ADVP': 3, 'I-ADVP': 4, 'B-CONJP': 5, 'I-CONJP': \"\n",
      " \"6, 'B-INTJ': 7, 'I-INTJ': 8, 'B-LST': 9, 'I-LST': 10, 'B-NP': 11, 'I-NP': \"\n",
      " \"12, 'B-PP': 13, 'I-PP': 14, 'B-PRT': 15, 'I-PRT': 16, 'B-SBAR': 17, \"\n",
      " \"'I-SBAR': 18, 'B-UCP': 19, 'I-UCP': 20, 'B-VP': 21, 'I-VP': 22}], named \"\n",
      " \"entity recognition tags: [{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, \"\n",
      " \"'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}] from the \"\n",
      " \"following: '<generator object generate_corps.<locals>.<genexpr> at \"\n",
      " \"0x7fb140311f50>'.\")\n"
     ]
    }
   ],
   "source": [
    "single_sentence = tuple(generate_corps(1, \"train\"))\n",
    "single_prompt = prompt_4(single_sentence, pos_tags, chunk_tags, ner_tags)\n",
    "\n",
    "response = perform_request(single_prompt)\n",
    "# pprint(response)\n",
    "# pprint((json.loads(response[\"choices\"][0][\"message\"][\"content\"]), single_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make multiprocessing request to make the responces from model faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corps = tuple(tuple(generate_corps(5, \"train\")) for _ in range(5))\n",
    "corps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = tuple(prompt_5(corp, pos_tags, chunk_tags, ner_tags) for corp in corps )\n",
    "with multiprocessing.Pool(10) as p: \n",
    "    responses = p.map(perform_request, prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform into dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'entities': [{'chunk_tag': 'B-NP',\n",
      "                 'entity': 'The',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'DT'},\n",
      "                {'chunk_tag': 'I-NP',\n",
      "                 'entity': 'American Stock Exchange',\n",
      "                 'ner_tag': 'B-ORG',\n",
      "                 'pos_tag': 'NNP'},\n",
      "                {'chunk_tag': 'B-VP',\n",
      "                 'entity': 'said',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'VBD'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'entity': 'there',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'EX'},\n",
      "                {'chunk_tag': 'I-VP',\n",
      "                 'entity': 'was',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'VBD'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'entity': 'no after-hours activity',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'NN'}]},\n",
      "  'Peace talks between the two sides were last held in February .'),\n",
      " ({'entities': [['Mauritius', 'B-LOC', 'NNP'],\n",
      "                ['now', 'O', 'RB'],\n",
      "                ['play', 'O', 'VB'],\n",
      "                ['in', 'O', 'IN'],\n",
      "                ['group', 'O', 'NN'],\n",
      "                ['seven', 'O', 'CD'],\n",
      "                ['of', 'O', 'IN'],\n",
      "                ['the', 'O', 'DT'],\n",
      "                ['qualifiers', 'O', 'NNS'],\n",
      "                ['against', 'O', 'IN'],\n",
      "                ['Malawi', 'B-LOC', 'NNP'],\n",
      "                [',', 'O', ','],\n",
      "                ['Mozambique', 'B-LOC', 'NNP'],\n",
      "                ['and', 'O', 'CC'],\n",
      "                ['favourites', 'O', 'NNS'],\n",
      "                ['Zambia', 'B-LOC', 'NNP'],\n",
      "                ['.', 'O', '.']]},\n",
      "  'ATLANTA AT PITTSBURGH'),\n",
      " ({'entities': [{'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'B-LOC',\n",
      "                 'pos_tag': 'NNP',\n",
      "                 'text': 'Arkansas'},\n",
      "                {'chunk_tag': 'B-VP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'VBZ',\n",
      "                 'text': 'has'},\n",
      "                {'chunk_tag': 'I-VP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'VBN',\n",
      "                 'text': 'been'},\n",
      "                {'chunk_tag': 'I-VP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'VBN',\n",
      "                 'text': 'spared'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'DT',\n",
      "                 'text': 'the'},\n",
      "                {'chunk_tag': 'I-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'NN',\n",
      "                 'text': 'loss'},\n",
      "                {'chunk_tag': 'B-PP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'IN',\n",
      "                 'text': 'of'},\n",
      "                {'chunk_tag': 'B-ADVP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'RB',\n",
      "                 'text': 'predominantly'},\n",
      "                {'chunk_tag': 'I-ADVP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'JJ',\n",
      "                 'text': 'black'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'NNS',\n",
      "                 'text': 'churches'},\n",
      "                {'chunk_tag': 'I-PP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'TO',\n",
      "                 'text': 'to'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'NN',\n",
      "                 'text': 'arson'},\n",
      "                {'chunk_tag': 'O', 'ner_tag': 'O', 'pos_tag': ',', 'text': ','},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'DT',\n",
      "                 'text': 'a'},\n",
      "                {'chunk_tag': 'I-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'NN',\n",
      "                 'text': 'wave'},\n",
      "                {'chunk_tag': 'B-SBAR',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'WDT',\n",
      "                 'text': 'that'},\n",
      "                {'chunk_tag': 'B-VP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'VBZ',\n",
      "                 'text': 'has'},\n",
      "                {'chunk_tag': 'I-VP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'VBN',\n",
      "                 'text': 'claimed'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'DT',\n",
      "                 'text': 'an'},\n",
      "                {'chunk_tag': 'I-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'VBN',\n",
      "                 'text': 'estimated'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'CD',\n",
      "                 'text': '30'},\n",
      "                {'chunk_tag': 'I-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'NNS',\n",
      "                 'text': 'houses'},\n",
      "                {'chunk_tag': 'B-PP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'IN',\n",
      "                 'text': 'of'},\n",
      "                {'chunk_tag': 'I-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'NN',\n",
      "                 'text': 'worship'},\n",
      "                {'chunk_tag': 'B-PP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'IN',\n",
      "                 'text': 'across'},\n",
      "                {'chunk_tag': 'I-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'DT',\n",
      "                 'text': 'the'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'B-LOC',\n",
      "                 'pos_tag': 'NN',\n",
      "                 'text': 'south'},\n",
      "                {'chunk_tag': 'I-PP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'IN',\n",
      "                 'text': 'in'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'DT',\n",
      "                 'text': 'the'},\n",
      "                {'chunk_tag': 'I-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'JJ',\n",
      "                 'text': 'past'},\n",
      "                {'chunk_tag': 'B-ADJP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'JJ',\n",
      "                 'text': 'several'},\n",
      "                {'chunk_tag': 'I-NP',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': 'NNS',\n",
      "                 'text': 'months'},\n",
      "                {'chunk_tag': 'O',\n",
      "                 'ner_tag': 'O',\n",
      "                 'pos_tag': '.',\n",
      "                 'text': '.'}]},\n",
      "  'W L PCT GB'),\n",
      " ({'entities': [{'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'B-PER',\n",
      "                 'pos_tag': 'NNP',\n",
      "                 'text': 'Richard Lamm'},\n",
      "                {'chunk_tag': '',\n",
      "                 'ner_tag': '',\n",
      "                 'pos_tag': '',\n",
      "                 'text': 'has decided not to endorse Ross Perot as the '\n",
      "                         'presidential candidate of the Reform Party , CNN '\n",
      "                         'reported late Tuesday .'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'B-PER',\n",
      "                 'pos_tag': 'NNP',\n",
      "                 'text': 'Ross Perot'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'B-ORG',\n",
      "                 'pos_tag': 'NNP',\n",
      "                 'text': 'Reform Party'},\n",
      "                {'chunk_tag': 'B-NP',\n",
      "                 'ner_tag': 'B-ORG',\n",
      "                 'pos_tag': 'NNP',\n",
      "                 'text': 'CNN'}]},\n",
      "  'DETROIT 45 82 .354 27 1/2'),\n",
      " ({'tokens': [{'chunk_tag': 'B-NP',\n",
      "               'ner_tag': 'B-LOC',\n",
      "               'pos_tag': 'NNP',\n",
      "               'word': 'August'},\n",
      "              {'chunk_tag': 'I-NP',\n",
      "               'ner_tag': 'O',\n",
      "               'pos_tag': 'CD',\n",
      "               'word': '27'},\n",
      "              {'chunk_tag': 'B-VP',\n",
      "               'ner_tag': 'O',\n",
      "               'pos_tag': 'VB',\n",
      "               'word': 'fix'},\n",
      "              {'chunk_tag': 'B-NP',\n",
      "               'ner_tag': 'B-LOC',\n",
      "               'pos_tag': 'NNP',\n",
      "               'word': 'August'},\n",
      "              {'chunk_tag': 'I-NP',\n",
      "               'ner_tag': 'O',\n",
      "               'pos_tag': 'CD',\n",
      "               'word': '26'},\n",
      "              {'chunk_tag': 'B-VP',\n",
      "               'ner_tag': 'O',\n",
      "               'pos_tag': 'VB',\n",
      "               'word': 'fix'}]},\n",
      "  '1. Chandra Sturrup ( Bahamas ) 11.34 seconds')]\n"
     ]
    }
   ],
   "source": [
    "pprint([(json.loads(response[\"choices\"][0][\"message\"][\"content\"] ), corp) for response, corp in zip(responses, corps)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
