{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NERC task with CoNLL 2003\n",
    "\n",
    "This is the first part of the task dedicated to reserching *Named Entity Recognition and Classification (NERC)* task applying to the *CoNLL 2003 dataset*. In this step we need find the best way to generate markup for the dataset in the subsequent steps. Also, we will experiment with prompt engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis\n",
    "\n",
    "First of all, we need to import necessery libraries and download our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import json\n",
    "import random\n",
    "import multiprocessing\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://llm.ispras.ru/api/chat/completions\"\n",
    "API_KEY = \"YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"eriktks/conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the information about the dataset. We have three splits: 'train', 'validation', 'test'.\n",
    "\n",
    "`dataset.keys()` - to see what splits we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_tags': [11, 0, 11, 21, 11, 12, 0, 11, 13, 11, 12, 0],\n",
      " 'id': '0',\n",
      " 'ner_tags': [0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      " 'pos_tags': [21, 8, 22, 37, 22, 22, 6, 22, 15, 12, 21, 7],\n",
      " 'tokens': ['SOCCER',\n",
      "            '-',\n",
      "            'JAPAN',\n",
      "            'GET',\n",
      "            'LUCKY',\n",
      "            'WIN',\n",
      "            ',',\n",
      "            'CHINA',\n",
      "            'IN',\n",
      "            'SURPRISE',\n",
      "            'DEFEAT',\n",
      "            '.']}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[\"test\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train':  14041 examples.\n",
      "Split 'validation':  3250 examples.\n",
      "Split 'test':  3453 examples.\n"
     ]
    }
   ],
   "source": [
    "for split in dataset.keys():\n",
    "    dataset_split = dataset[split]\n",
    "    split_len = len(dataset_split)\n",
    "    print(f\"Split '{split}':  {split_len} examples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Tags\n",
    "\n",
    "The original dataset uses named entity recognition (NER) tags in the IOB2 format.\n",
    "\n",
    "Each token is annotated with three types of tags:\n",
    "1. POS Tags: Indicate the token's grammatical role (e.g., 'NN', 'VB', etc.).\n",
    "2. Chunk Tags: Specify the syntactic chunk the token belongs to (e.g., 'B-NP', 'I-NP').\n",
    "3. NER Tags: Identify named entities using the IOB2 scheme:\n",
    "   - 'O'      : Token is not part of any entity.\n",
    "   - 'B-PER'  : Beginning of a person entity.\n",
    "   - 'I-PER'  : Inside a person entity.\n",
    "   - 'B-ORG'  : Beginning of an organization entity.\n",
    "   - 'I-ORG'  : Inside an organization entity.\n",
    "   - 'B-LOC'  : Beginning of a location entity.\n",
    "   - 'I-LOC'  : Inside a location entity.\n",
    "   - 'B-MISC' : Beginning of a miscellaneous entity.\n",
    "   - 'I-MISC' : Inside a miscellaneous entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = {'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9, 'CC': 10, 'CD': 11, 'DT': 12,\n",
    " 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18, 'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23,\n",
    " 'NNS': 24, 'NN|SYM': 25, 'PDT': 26, 'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33,\n",
    " 'SYM': 34, 'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42, 'WDT': 43,\n",
    " 'WP': 44, 'WP$': 45, 'WRB': 46}\n",
    "\n",
    "chunk_tags = {'O': 0, 'B-ADJP': 1, 'I-ADJP': 2, 'B-ADVP': 3, 'I-ADVP': 4, 'B-CONJP': 5, 'I-CONJP': 6, 'B-INTJ': 7, 'I-INTJ': 8,\n",
    " 'B-LST': 9, 'I-LST': 10, 'B-NP': 11, 'I-NP': 12, 'B-PP': 13, 'I-PP': 14, 'B-PRT': 15, 'I-PRT': 16, 'B-SBAR': 17,\n",
    " 'I-SBAR': 18, 'B-UCP': 19, 'I-UCP': 20, 'B-VP': 21, 'I-VP': 22}\n",
    "\n",
    "ner_tags = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
