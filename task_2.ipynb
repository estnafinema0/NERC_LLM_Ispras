{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2\n",
    "\n",
    "On this stage we want to compare the performance of a small neural model trained (BERT) on two different types of annotations: \n",
    "    (1) annotations generated using the best method from the first stage of the project,\n",
    "    (2) the original, ground-truth annotations provided in the Coll2003 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import trange, tqdm\n",
    "from dataclasses import dataclass\n",
    "import pathlib\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://llm.ispras.ru/api/chat/completions\"\n",
    "API_MODEL_URL = \"https://llm.ispras.ru/api/models\"\n",
    "API_KEY = \"YOUR_TOKEN\"\n",
    "with open('./secrets') as file:\n",
    "    data: dict = json.load(file)\n",
    "    API_KEY = data.get('API_KEY', 'FAILED TO LOAD')\n",
    "\n",
    "HIDDEN = 512\n",
    "NER_TAGS = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "ADDITIONAL = {\"[CLS]\": 9, \"[SEP]\": 10, \"X\": 11}\n",
    "NER_TAGS.update(ADDITIONAL)\n",
    "TAGS_COUNT = len(NER_TAGS)\n",
    "DATABASE_DIR = './data'\n",
    "EMBEDDINGS_PATH = f'{DATABASE_DIR}/embeddings.txt'\n",
    "CWD = '~/Рабочий стол/NERC_LLM_Ispras/data'\n",
    "BERT_MODEL = 'bert-base-cased'\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initalizing Neural Network Model\n",
    "\n",
    "Model will consist of: \n",
    "    (1) *pre-trained BERT model* for word embeddings, which captures contextual information effectively. \n",
    "    (2) *bidirectional LSTM (BiLSTM) layer* to process the sequence of embeddings and capture dependencies between words. \n",
    "    (3) *linear layer* to map the LSTM outputs to the NER tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERSmall(nn.Module):\n",
    "    def __init__(self, model_name, embedding_dim=300):\n",
    "        super(NERSmall, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "        self.bilstm1 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=HIDDEN,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        # self.bilstm2 = nn.LSTM(\n",
    "        #     input_size=HIDDEN * 2,\n",
    "        #     hidden_size=HIDDEN * HIDDEN,\n",
    "        #     bidirectional=True,\n",
    "        # )\n",
    "        self.lin = nn.Linear(HIDDEN * 2, TAGS_COUNT)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.bert(x)\n",
    "        y, _ = self.bilstm1(y)\n",
    "        return self.lin(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and Loading Data\n",
    "\n",
    "To organize the data, we define a simple data structure `InputExample`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputExample:\n",
    "    guid: str\n",
    "    text: str\n",
    "    ner_tag: str | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make `parse_dataformat` function to processe raw text data, splitting it into sentences and their corresponding NER tags. We make it handle empty lines and document separators. It organizes the data into a list of InputExample objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataformat(base_data: str, set_type: str) -> list[InputExample]:\n",
    "    data = []\n",
    "    sentence = []\n",
    "    ner_tags = []\n",
    "    for line in base_data.splitlines():\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                data.append((sentence, ner_tags))\n",
    "                sentence = []\n",
    "                ner_tags = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append(splits[0])\n",
    "        ner_tags.append(splits[-1][:-1])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        data.append((sentence, ner_tags))\n",
    "        sentence = []\n",
    "        ner_tags = []\n",
    "\n",
    "    return [InputExample(f'{set_type}-{i}', ' '.join(sentence), ner_tag) for i, (sentence, ner_tag) in enumerate(data)]\n",
    "\n",
    "def load_examples(data_dir) -> tuple[list[InputExample], list[InputExample], list[InputExample]]:\n",
    "    with open(pathlib.Path.joinpath(pathlib.Path(data_dir), 'test.txt')) as test, open(pathlib.Path.joinpath(pathlib.Path(data_dir), 'train.txt')) as train, open(pathlib.Path.joinpath(pathlib.Path(data_dir), 'valid.txt')) as valid:\n",
    "        test_data, train_data, valid_data = test.read(), train.read(), valid.read()\n",
    "    return parse_dataformat(test_data, 'test'), parse_dataformat(train_data, 'train'), parse_dataformat(valid_data, 'valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create `NERDataSet` class that prepares the data for training. It tokenizes the input text, maps NER tags to their corresponding IDs, and pads sequences to a fixed length. The __getitem__ method processes each example by adding special tokens [CLS] and [SEP], tokenizing the text, and creating attention masks and sentence IDs. The method returns tensors for input IDs, NER tag IDs, attention masks, sentence IDs, and tag masks, which are used by the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataSet(Dataset):\n",
    "    def __init__(self, data: list[InputExample], tokenizer: BertTokenizer, ner_tag_map: dict[str, int],\n",
    "                 max_len: int = 128):\n",
    "        self._max_len = max_len\n",
    "        self._ner_tag_map = ner_tag_map\n",
    "        self._data = data\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_example = self._data[idx]\n",
    "        text = input_example.text\n",
    "        ner_tags = input_example.ner_tag\n",
    "        word_tokens = ['[CLS]']\n",
    "        ner_tag_list = ['[CLS]']\n",
    "        ner_tag_mask = [0]  # value in {0,1} -- 0 signifies invalid token\n",
    "\n",
    "        input_ids = [self._tokenizer.convert_tokens_to_ids('[CLS]')]\n",
    "        ner_tag_ids = [self._ner_tag_map['[CLS]']]\n",
    "\n",
    "        for word, ner_tag in zip(text.split(), ner_tags):\n",
    "            tokenized_word = self._tokenizer.tokenize(word)\n",
    "\n",
    "            word_tokens.extend(tokenized_word)\n",
    "            assert len(tokenized_word) > 0\n",
    "            input_ids.extend(map(self._tokenizer.convert_tokens_to_ids, tokenized_word))\n",
    "\n",
    "            ner_tag_list.append(ner_tag)\n",
    "            ner_tag_ids.append(self._ner_tag_map[ner_tag])\n",
    "            ner_tag_mask.append(1)\n",
    "            # len(tokenized_word) > 1 only if it splits word in between, in which case\n",
    "            # the first token gets assigned NER tag and the remaining ones get assigned\n",
    "            # X\n",
    "            ner_tag_list.extend(itertools.repeat('X', len(tokenized_word) - 1))\n",
    "            ner_tag_ids.extend(itertools.repeat(self._ner_tag_map['X'], len(tokenized_word) - 1))\n",
    "            ner_tag_mask.extend(itertools.repeat(0, len(tokenized_word) - 1))\n",
    "\n",
    "        assert len(word_tokens) == len(ner_tag_list) == len(input_ids) == len(ner_tag_ids) == len(ner_tag_mask)\n",
    "\n",
    "        if len(word_tokens) >= self._max_len:\n",
    "            word_tokens = word_tokens[:(self._max_len - 1)]\n",
    "            ner_tag_list = ner_tag_list[:(self._max_len - 1)]\n",
    "            input_ids = input_ids[:(self._max_len - 1)]\n",
    "            ner_tag_ids = ner_tag_ids[:(self._max_len - 1)]\n",
    "            ner_tag_mask = ner_tag_mask[:(self._max_len - 1)]\n",
    "\n",
    "        assert len(word_tokens) < self._max_len, len(word_tokens)\n",
    "\n",
    "        word_tokens.append('[SEP]')\n",
    "        ner_tag_list.append('[SEP]')\n",
    "        input_ids.append(self._tokenizer.convert_tokens_to_ids('[SEP]'))\n",
    "        ner_tag_ids.append(self._ner_tag_map['[SEP]'])\n",
    "        ner_tag_mask.append(0)\n",
    "\n",
    "        assert len(word_tokens) == len(ner_tag_list) == len(input_ids) == len(ner_tag_ids) == len(ner_tag_mask)\n",
    "\n",
    "        sentence_id = [0] * len(input_ids)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        left = self._max_len - len(input_ids)\n",
    "        input_ids.extend(itertools.repeat(0, left))\n",
    "        ner_tag_ids.extend(itertools.repeat(self._ner_tag_map['X'], left))\n",
    "        attention_mask.extend(itertools.repeat(0, left))\n",
    "        sentence_id.extend(itertools.repeat(0, left))\n",
    "        ner_tag_mask.extend(itertools.repeat(0, left))\n",
    "\n",
    "\n",
    "        assert len(word_tokens) == len(ner_tag_list)\n",
    "        assert len(input_ids) == len(ner_tag_ids) == len(attention_mask) == len(sentence_id) == len(\n",
    "            ner_tag_mask) == self._max_len, len(input_ids)\n",
    "        return torch.LongTensor(input_ids), torch.LongTensor(ner_tag_ids), torch.LongTensor(\n",
    "            attention_mask), torch.LongTensor(sentence_id), torch.BoolTensor(ner_tag_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Environment\n",
    "\n",
    "We are using BERT Tokenizer from `from_pretrained`.\n",
    "\n",
    "The dataset is loaded into training, validation, and test sets using the load_examples function. DataLoader objects are created for each dataset, enabling efficient batching and shuffling of the data during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BERT_MODEL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mBERT_MODEL\u001b[49m)\n\u001b[1;32m      3\u001b[0m test, train, valid \u001b[38;5;241m=\u001b[39m load_examples(DATABASE_DIR)\n\u001b[1;32m      4\u001b[0m train_iter \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mtrain, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BERT_MODEL' is not defined"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "test, train, valid = load_examples(DATABASE_DIR)\n",
    "train_iter = DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "eval_iter = DataLoader(dataset=valid, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_iter = DataLoader(dataset=test, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
